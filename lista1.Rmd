---
title: Lista 1 de exercícios
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---
***

```{r setup, include=FALSE}
# <r code> ========================================================== #
library(knitr)

opts_chunk$set(cache=TRUE
               , cache.path="cache/"
               , fig.path="iBagens/"
               , dpi=100
               , fig.align="center"
               , comment=NA
               , warning=FALSE
               , error=FALSE
               , message=FALSE)

options(width=100)
# </r code> ========================================================= #
```

# Exercício 1
***

**A distribuição de Pareto ($\alpha,\beta$) tem a seguinte 
  distribuição**

\[ F(x) = 1-(\beta /x)^\alpha \]

**Expressão na qual x >=b, a>0. Apresente um algorítimo para geração de
  variáveis aleatórias para esta distribuição. Usando R, simule uma
  amostra para Pareto(2,2). Apresente o histograma da amostra combinado
  com a densidade teórica da mesma para comparação.**

**Solução:**

O algorítimo para geração de variáveis aleatórias para esta
distribuição se baseia na geração de variáveis aleatórias pelo método 
da inversão.

Segundo o teorema da Probabilidade da Transformação Integral
(Rizzo,2008), se X é uma variável aleatória continua com distribuição 
\(F_x (x)\), então U = \(F_x (x)$ ~ U (0,1)\).

Assim, 

\[ F_x^{-1}(u)= inf [x : F_x (x)=u] , 0<u<1.\]


Calculando a inversa de F(x), temos que $F_x^{-1}(u)$ é descrita por:

$$x =b/(1-u)^{1/a}$$ 


O algorítimo para geração de um conjunto de valores aleatórios 
uniformes, então,

```{r}
func.inver <- function(a,b,Nsimu = 1e6){
# Difine intervalos (a,b) e gera Nsimu variáveis

  # Gerar Nsimu uniformes (a,b)
  u <-runif(Nsimu)  

  # Gerar Nsimu variáveis aleatórias pela inversão
  gx = b/((1-u)^(1/a))
  
  return(gx)
  
}
```

A partir desta função, gerou-se um número grande variáveis aleatórias 
(u), e substituiu-se na função inversa:

```{r}
set.seed(42017)
X = func.inver(2,2)
```

O histograma desta função, com os parâmetros dados, temos:

```{r}
hist(X)
```

Como a Pareto é uma distribuição para valores extremos, visualizações 
de toda a amostra gerada por um histograma não é muito informativa, 
pois a maior observação chega a ter valor `r max(X)`. Teoricamente, 
quando `x = 20`, temos que $F(20) = 0.99$. De fato, nas `1e6` variáveis
geradas, `r as.numeric(table(X>20)[2]/1e6)*100`% são maiores que $20$.
Assim, vamos representar o histograma até o valor `x=20` para que 
possamos ter uma melhor visualização dos dados.

Em seguida, define-se a densidade de Pareto

```{r}
  # Densidade da Pareto(2,2)
  dpareto=function(x, a=2, b=2) a*b^a/x^(a+1)
```

Por fim, comparando-se o histograma das duas distribuições, a teórica e
a estimada, temos:

```{r}
# Histograma das variáveis geradas
hist(subset(X, X <= 20)) 
# Curva da distribuição teórica da Pareto(2,2)
lines(2:20, dpareto(2:20)*1e6, col = "red")
```

# Exercício 2
***

**Prove que variáveis aleatórias geradas pelo método de 
  aceitação-rejeição são, de fato, uma amostra aleatória da
  densidade-alvo \(f_{X}\).**

**Solução:**

Geramos amostras de uma distribuição de interesse a partir da geração
de uma distribuição conhecida e então, rejeitando um subconjunto 
gerado. O mecanismo de rejeição é construído de forma que, a amostra
que será aceita segue a distribuição de interesse. Queremos gerar
valores aleatórios de uma densidade de probabilidade $f$ definida em
um algum conjunto $X$. Seja $g$ uma densidade em $X$ a partir da qual
sabemos gerar valores aleatórios e com a propriedade de que
$f(X) < c*g(X)$, para todo $x \in X$ e alguma constante $c$.

Geramos um elemento $x$ a partir da densidade $g$ e aceitamos o
elemento com probabilidade $f(x)/cg(X)$, ou seja,

$$
P(\text{Aceitar}|Y) = P\left(U < \frac{f(Y)}{cg(Y)}|Y\right) = 
\frac{f(Y)}{cg(Y)}.
$$

Note ainda que a probabilidade total de aceitar $Y$ pode ser calculada 
da seguinte maneira:

$$
\sum_{n}P(\text{Aceitar}|y)P(Y=y)=\sum_{n}\frac{f(y)}{cg(y)}g(y)=
\frac{1}{c}.
$$

Agora possuimos todos os elementos para mostrar, pelo menos no caso 
discreto, que o método de aceitação e rejeição gera, de fato, números 
aleatórios da distribuição de interesse. Para a prova utilizamos o 
teorema de Bayes.

Suponha um $k$ qualquer de modo que $f(k)>0$, então pelo teorema de 
Bayes temos que: 

$$
P(k|\text{Aceitar})=\frac{P(\text{Aceitar}|k)g(k)}{P(\text{Aceitar})} =
\frac{[f(k)/(cg(k))]g(k)]}{1/c} = f(k)
$$

Este método pode ser implementado ao gerar um elemento $u$ distribuido 
uniformemente em (0,1) e então, aceitamos se $u < f(x)/cg(x)$. 
Se $x$ é rejeitado, um novo candidato é amostrado de $g$ e repetimos o 
teste de aceitação. 
O processo se repete até que o teste de aceitação gere a quantidade 
desejada de números aleatórios; os valores aceitos são elementos com 
distribuição $f$.

Algoritmo

O algoritmo do método de aceitação-rejeição pode ser apresentado nos 
passos abaixo.

1. Simula-se o valor $x$ com densidade $g$ }

2. Gera-se um número aleatório $u \sim \text{Unif}(0,1)$}

3. Se $u < f(x)/cg(x)$, aceita-se o valor e para. Caso contrário, 
   retorna-se ao passo 1.

# Exercício 3
***

**O núcleo escalonado de Epanechnikov é**

\[ f_{e}(x) = \frac{3}{4} (1 - x{2}),\quad |x| < 1. \]

**Uma proposta de geração de varáveis aleatórias para este caso é:**

a. \(U_{1}, U_{2}, u_{3} \sim U(-1, 1)\)

b. **Se** \(|U_{3}| > |U_{2}|\) **e** \(|U_{3}| > |U_{1}|\), 
   **retorne** \(U_{2}\). **Caso contrário, retorne** \(U_{3}\).

**Solução:**

No código a seguir, simulamos uma amostra da variável Epanechnikov 
usando o algoritmo descrito na questão e, em seguida, construimos um 
histograma das variáveis geradas. 

```{r}
# densidade Epanechnikov
d_epach = function(x) 3*(1 - x^2)/4
# função para gerar variáveis aleatórias com distribuição Epanechnikov
r_epach = function(n){
  v = rep(0,n)
  for(i in 1:n){
    u1 = runif(1,-1,1); u2 = runif(1,-1,1); u3 = runif(1,-1,1);
    if((abs(u3)>abs(u2))&&(abs(u3)>abs(u1))){
      v[i] = u2
    }else{
      v[i] = u3
    }
  }
  return(v)
}

# histograma de uma amostra da Epanechnikov e a densidade verdadeira
hist(r_epach(100000),prob=TRUE,
     main="Histograma - núcleo de Epanechnikov",
     ylim=c(0,1),ylab="Densidade",xlab="")
curve(d_epach, xlim=c(-1,1),add=TRUE,col=2,lwd=2)
```

Comparando o histograma com a densidade Epanechnikov, vemos que os 
valores simulados estão bastante próximos da densidade teórica.

Nessa parte vamos provar que o algoritmo de fato gera valores da 
densidade Epanechnikov, primeiro utilizamos integrais para encontrar a 
função de distribuição desejada e verificamos que o algoritmo é 
adequado. Depois resolvemos de outro método, utilizando estatística de 
ordem para facilitar a resolução . 

1º Método

Sabemos que $U_i \sim U(-1,1)$, $i\in\{1,2,3\}$, daí a função de 
distribuição de $U_i$ é 

$$
F_U(u) = \frac{u+1}{2},
$$

então, para $Y_i = |U_i|$ temos que

$$
F_{Y_i}(y) = P(|U_i| \leq y) = P(-y \leq U_i \leq y) = F_{U_i}(y) - 
F_{U_i}(-y) = \left(\frac{y+1}{2}\right) - \left(\frac{-y+1}{2}\right) 
= y,
$$

ou seja, $Y_i \sim U(0,1)$, $i\in\{1,2,3\}$.

Sendo $X$ a variável aleatória gerada de acordo com o algoritmo 
descrito na questão, temos que

$$
X = \left\{\begin{array}{cc}
U_2 & \textrm{se } |U_3|>max\{|U_1|,|U_2|\}\\
U_3 & \textrm{se } |U_3|\leq max\{|U_1|,|U_2|\}\\
\end{array}\right.,
$$

e como $U_2$ e $U_3$ são simétricas em torno de zero, pelo algoritmo 
temos que $X$ também o será. Logo, $P(X<0) = 1/2$. Considerando então 
$0<x<1$, temos que

\begin{align*}
P(|X|\leq x) &= P(|U_3|>max\{|U_1|,|U_2|\},|U_2|\leq x) + P(|U_3| \leq
max\{|U_1|,|U_2|\},|U_3|\leq x)\\

&= P(Y_3>max\{Y_1,Y_2\},Y_2\leq x) + P(Y_3 \leq max\{Y_1,Y_2\},Y_3\leq 
x).
\end{align*}

Sabemos que a densidade de $Y_i$ é $f_{Y_i}(y) = 1$ para $0<y<1$. 
Assim, calculando a primeira probabilidade temos que

\begin{align*}
P(Y_3>max\{Y_1,Y_2\},Y_2\leq x) &= P(Y_1\leq Y_2 \leq Y_3, Y_2\leq x) +
P(Y_2\leq Y_2 \leq Y_3, Y_2\leq x)\\\
&= \int_{0}^{x}\int_{0}^{y_2}\int_{y_2}^{1}dy_3dy_1dy_2 + 
\int_{0}^{x}\int_{y_2}^{1}\int_{y_1}^{1}dy_3dy_1dy_2\\
&= \int_{0}^{x}\int_{0}^{y_2}(1-y_2)dy_1dy_2 + 
\int_{0}^{x}\int_{y_2}^{1}(1-y_1)dy_1dy_2\\
&= \int_{0}^{x}(1-y_2)y_2dy_1dy_2 + 
\int_{0}^{x}\left[\left.(y_1-\frac{y_1^2}{2})\right|_{y_2}^{1}\right]
dy_2\\
&= \left.\left(\frac{y_2^2}{2}-\frac{y_2^3}{3}\right)\right|_0^x + 
\int_{0}^{x}\left[1 - \frac{1}{2} - y_2 + \frac{y_2^2}{2} \right]dy_2\\
&= \frac{x^2}{2}-\frac{x^3}{3} + \left.\left[\frac{y_2}{2} - 
\frac{y_2^2}{2} + \frac{y_2^3}{6} \right]\right|_0^x\\
&= \frac{x^2}{2}-\frac{x^3}{3} + \frac{x}{2}-\frac{x^2}{2} + 
\frac{x^3}{6} = \frac{x}{2} - \frac{x^3}{6}\\
\end{align*}

Na segunda probabilidade, temos que

\begin{align*}
P(Y_3 \leq max\{Y_1,Y_2\},Y_3\leq x) &= P(Y_3\leq Y_2 \leq Y_1, Y_3\leq
x) + P(Y_2\leq Y_3 \leq Y_1, Y_3\leq x) \\
&+ P(Y_3\leq Y_1 \leq Y_2, Y_3\leq x) + P(Y_1\leq Y_3 \leq Y_2, Y_3\leq
x)\\
&= 2[P(Y_3\leq Y_2 \leq Y_1, Y_3\leq x) + P(Y_2\leq Y_3 \leq Y_1, 
Y_3\leq x)],
\end{align*}

onde na primeira igualdade listamos as possíveis formas que o evento 
$[Y_3>max\{Y_1,Y_2\},Y_3\leq x]$ pode ocorrer e na segunda igualdade 
usamos o fato de $Y_1$ e $Y_2$ serem identicamente distribuídas. Daí 

\begin{align*}
P(Y_3 \leq max\{Y_1,Y_2\},Y_3\leq x) &= 2\left[ 
\int_{0}^{x}\int_{y_3}^{1}\int_{y_2}^{1}dy_1dy_2dy_3 + 
\int_{0}^{x}\int_{0}^{y_3}\int_{y_3}^{1}dy_1dy_2dy_3 \right]\\
&= 2\left[ \int_{0}^{x}\int_{y_3}^{1}(1-y_2)dy_2dy_3 + 
\int_{0}^{x}\int_{0}^{y_3}(1-y_3)dy_2dy_3 \right]\\
&= 2\left[
\int_{0}^{x}\left[\left.(y_2-\frac{y_2^2}{2})\right|_{y_3}^{1}\right]
dy_3 + \int_{0}^{x}(1-y_3)y_3dy_3 \right]\\
&= 2\left[ \int_{0}^{x}\left[\frac{1}{2} - y_3 + 
\frac{y_3^2}{2}\right]dy_3 + 
\left.\left(\frac{y_3^2}{2}-\frac{y_3^3}{3}\right)\right|_0^x \right]\\
&= 2\left[ \left.\left[\frac{y_3}{2} - \frac{y_3^2}{2} + 
\frac{y_3^3}{6} \right]\right|_0^x + \frac{x^2}{2}-\frac{x^3}{3} 
\right]\\
&= 2\left[ \frac{x}{2} - \frac{x^2}{2} + \frac{x^3}{6} + 
\frac{x^2}{2}-\frac{x^3}{3} \right] = x - \frac{x^3}{3}\\
\end{align*}

Portanto,

$$
P(|X|\leq x) = \frac{x}{2} - \frac{x^3}{6} + x - \frac{x^3}{3} = 
\frac{3x}{2} - \frac{3x^3}{6} = \frac{3x - x^3}{2},
$$
e pela simetria de $X$, temos que 

$$
P(X\leq x) = P(X\leq 0) + P(0<X\leq x) = \frac{1}{2} + \frac{P(|X|\leq 
x)}{2} = \frac{1}{2} + \frac{3x - x^3}{4},
$$

onde $0<x<1$. Novamente usando o fato da distribuição de $X$ ser 
simétrica, temos para $0<x<1$ que

$$
F_X(-x) = 1 - F_X(x) = 1 - \left[ \frac{1}{2} + \frac{3x - x^3}{4} 
\right] 
= \frac{1}{2} - \frac{(3x - x^3)}{4} 
= \frac{1}{2} + \frac{3(-x) - (-x)^3}{4},
$$

ou seja, a função de distribuição de $X$ é
$F_X(x) = 1/2 + (3x - x^3)/4$, para $x\in[-1,1]$, e derivando $F_X(x)$ 
em relação a $x$ obtemos a função densidade de $X$, que é

$$
f_X(x) = \frac{3(1-x^2)}{4}, \quad -1\leq x \leq 1,
$$

assim, vemos que $X$ tem distribuição Epanechnikov, e o método de 
geração está correto.

2º Método

Note que o algoritmo proposto pode ser encarado como uma mistura de 
distribuições das estatísticas de ordem 1 e 2 para três uniformes com 
parametrização $(0,1)$.

Portanto, é razoável afirmar que a distribuição desejada pode ser 
obtida da seguinte forma:

\begin{equation}\label{eq:estordem}
f^{\ast}(u)=\frac{1}{2}f_{(1)}(u)+\frac{1}{2}f_{(2)}(u).
\end{equation}
Note que está é uma afirmação, que será provada verdadeira caso a 
distribuição obtida seja, de fato, a que buscamos

Sabemos que a distribuição genérica de uma estatística de ordem 
qualquer é dada da seguinte maneira

$$
f_{(k)}(x) = \frac{n!}{(k-1)!(n-k)!} \left[F_{X}(x)\right]^{k-1} 
\left[1-F_{X}(x)\right]^{n-k}f_{X}(x)
$$

Adaptando para o nosso caso temos que:

$$
f_{(1)}(u)=3(1-u)^{2}
$$

e

$$
f_{(2)}(u)=3u(1-u),
$$

isso porque para uma distribuição uniforme (0,1),

$$
F_{X}=\frac{x-a}{b-a}=x
$$

e

$$
f_{X}=\frac{1}{b-a}=1
$$

Substituindo $f_{(1)}$ e $f_{(2)}$ na equação \eqref{eq:estordem} temos que:

\begin{equation}\label{eq:densepac}
f^{\ast}(u)=\frac{3}{2}(1-u)^{2} + \frac{3}{2}u(1-u) = 
\frac{3}{2}\left(1-u^2\right), u<1
\end{equation}

Observe que $u$ na equação acima está definida apenas para $u<1$, mas 
pela simetria da distribuição podemos dividir a equação 
\eqref{eq:densepac} por 2 para obter a função de distribuição no 
domínio desejado, que é $|u|<1$.

$$
f^{\ast}(u)=\frac{\frac{3}{2}(1-u)^{2} + \frac{3}{2}u(1-u)}{2} = 
\frac{3}{4}\left(1-u^2\right), |u|< 1
$$

# Exercício 4
***

** Seja $X$ uma variável aleatória não-negativa com
   $\mu = E(X) < \infty$. Para uma amostra aleatória
   $x_1, x_2, \ldots, _n$ da distribuição X, a razão de gini é:**

$$G = \dfrac{1}{2n^2}\sum_{j=1}^{n}\sum_{i =1}^{n}|x_i - x_j|.$$

**Esta quantidade é utilizada em Economia para medir a desigualdade na
  distribuição de renda. Observe  também que $G$ pode ser reescrito
  como função das estatísticas de ordem:**

$$G = \dfrac{1}{n^2\mu}\sum_{i=1}^{n}(2i - n -1)x_{(i)}.$$

**Se a média é desconhecida, considere $\hat G$ como a estatística $G$,   na qual subistitui-se $\mu$ por $\bar X$. Estime por simulação a
  média, a mediana e os decis(quantis 0,10,...,80,90,100) de $\hat G$,
  quanto $X$ tem:**

a. Distribuição lognormal;
b. Distribuição Uniforme;
c. Distribuição Bernoulli(0,1).

**Escolha os parâmetros das distribuições log-normal e uniforme    
  conforme achar adequado.**

**Solução:**

Nosso interesse é estimar $\mu = E(X)$ através de simulação 
considerando $3$ distribuições como referência:

a. $X \sim lognormal(0,1)$;
b. $X \sim Uniforme(0,1)$;
c. $X \sim Binomial(1,0,1)$.

O algoritmo é o seguinte:


1. Gerar $10^6$ valores da distribuição;
2. Ordena estes valores;
3. Substitui os valores ordenados na função $G$.
4. Calcula-se a média;
5. Repete este processo $n$ vezes;
6. Calcula-se a média das médias.

Seja a função gini abaixo com parâmetros n e dist, em que dist 
corresponde a qual função de refêrencia será usada, 1= logNormal, 
2=Uniforme e 3=Binomial.

```{r, include = TRUE}
gini=function(n,dist){
  if ((dist< 1) | (dist >3)) stop("Valor interios deve estar entre 1 e 3!")
  {gi=vector("numeric", n);
  if(dist==1){
    u=sort(rlnorm(n,0,1));
    xb=mean(u);
  } else 
    {if (dist==2){
      u=sort(runif(n,0,1))
      xb=mean(u)  
    }else 
      {if (dist==3){
        u=sort(rbinom(n,1,0.1))
        xb=mean(u)  
      }}}} 
  ki=0;
  for(i in 1:n){
    ki = ki + (2*i-n-1)*u[i];
  }
  return(ki/(n^2*xb))
}
```

Dado a função acima, podemos agora calcula a estimativa da média, 
mediana e os decis(quantis 0,10,...,80,90,100) de $\hat G$. 

* Seja $X \sim Lognormal(0,1)$

A estimativa da média de $G$ vai ser dado por
$E(X) = `r mean(replicate(1000, gini(10000,1)))`$. A estimativa da 
mediada é igual a `r median(replicate(1000,gini(10000,1)))` e os 
quantis são dados por:

```{r, include= TRUE}
decis <- matrix(round(quantile(replicate(1000, gini(1000,1)),seq(0,1,0.1)), 2), ncol = 11 )
colnames(decis) = c("  0%"  , "10%"   ,"20%",   "30%",   "40%",   "50%",
                    "60%",   "70%",   "80%", "90%",  "100%" )
rownames(decis) = c("Estimativas")
decis
```

* Seja $X \sim Uniforme(0,1)$

A estimativa da média de $G$ vai ser dado por
$E(X) = `r mean(replicate(1000, gini(10000,2)))`$. A estimativa da 
mediada é igual a `r median(replicate(1000,gini(10000,2)))` e os 
quantis são dados por:

```{r, include= TRUE}
decis <- matrix(round(quantile(replicate(1000, gini(1000,2)),seq(0,1,0.1)), 2), ncol = 11 )
colnames(decis) = c("  0%"  , "10%"   ,"20%",   "30%",   "40%",   "50%", 
                    "60%",   "70%",   "80%", "90%",  "100%" )
rownames(decis) = c("Estimativas")
decis
```

* Seja $X \sim Binomial(1,0.1)$

A estimativa da média de $G$ vai ser dado por
$E(X) = `r mean(replicate(1000, gini(10000,3)))`$. A estimativa da 
mediada é igual a `r median(replicate(1000,gini(10000,3)))` e os 
quantis são dados por:

```{r, include= TRUE}
decis <- matrix(round(quantile(replicate(1000, gini(10000,3)),seq(0,1,0.1)), 2), ncol = 11 )
colnames(decis) = c("  0%"  , "10%"   ,"20%",   "30%",   "40%",   "50%",
                    "60%",   "70%",   "80%", "90%",  "100%" )
rownames(decis) = c("Estimativas")
decis
```

# Exercício 5
***

**Construa um intervalo aproximado de confiança de 95% para a razão de
  Gini \(\gamma = E(G)\) se \(X\) segue uma distribuição log-normal com
  parâmetros desconhecidos. Averigue a taxa de cobertura do intervalo
  utilizando Monte Carlo.**

**Solução:**

Seja $X_j \sim LogNorm(\mu, \sigma^2)$, estamos interessados na
estatística $G_i = \frac{1}{n^2 \mu} \sum_{i=1}^n (2i - n - 1)x_{(i)}$.

Como a estatística de interesse é $G$, iremps gerar uma amostra de 
tamanho $100$ da mesma e então utilizar o Teorema do Limite Central 
(TLC) para criar o intervalo de confiança.
$$\frac{\bar{G} - E(G)}{\sqrt{var(\bar{G})}} \sim N(0, 1)$$

Para um nível de confiança de 95%, temos o seguinte intervalo de 
confiança para $E(G)$:

$$\Rightarrow IC(E(G), 0.95) = \bigg[\bar{G} - 
1.96*\sqrt{var(\bar{G})}, \bar{G} + 1.96*\sqrt{var(\bar{G})}\bigg]$$

Vamos gerar $1000$ intervalos de confiança para G. Portanto, devemos 
seguir o seguinte procedimento:

```{r}
# Para k = 1, 2, ..., 1000
## Para i = 1, 2, ..., 100
### Gerar uma amostra de tamanho n de X
### Calcular G_i
## Calcular Gbarra e S(G)
## Calcular o intervalo de confiança assintótico
## Verificar se a média real está dentro do intervalo
```

Ao final, a taxa de cobertura será dada pela média dos intervalos que 
cobriram o real valor.

O valor real de $E(G)$ é dado por:

$$E(G) = erf(\sigma/2)$$

# Exercício 6
***

**Use simulação de Monte Carlo para investigar a taxa empírica do Erro
  Tipo I do teste-*t*. Verifique se estes são aproximadamente iguais ao
  valor nominal \(\alpha\) quando a população amostrada não segue uma
  distribuição normal. Discuta estes casos quando as distribuições 
  seguem:**

* \(\chi^{2}\) (1)

* Uniforme (0, 2)

* Exponencial (1)

**Para cada caso, teste \(H_{0}: \mu = \mu_{0}\) vs.
  \(H_{1}: \mu \neq \mu_{0}\), onde \(\mu_{0}\) é a esperança de
  \(\chi^{2}\) (1), Uniforme (0, 2) e Exponencial (1), 
  respectivamente.**

**Solução:**

O teste-*t* é um teste utilizado para rejeitar ou não uma hipótese nula
quando a estatística de teste segue uma distribuição *t* de Student.

No nosso caso, faremos o teste sob \(H_{0}: \mu = 1\) vs.
\(H_{1}: \mu \neq 1\) e considerando a variância populacional
\(\sigma^{2}\) desconhecida. Com as parametrizações consideradas, todas
as distribuições estudadas tem esperança 1. Vamos verificar a taxa
empírica do Erro do Tipo I quando a população  não segue uma
distribuição normal.

Seja \(\ x_{1}, x_{2}, \dots, x_{n}\) valores amostrais da distribuição
estudada, então a estatística do teste será dada por:  

\[ T = \frac{\bar{X} - \mu}{S / \sqrt{n}} \sim t_{(n - 1)}. \]
 
Observe que rejeitamos \(H_{0}\) para grandes valores da estatística de
teste \(T\), indicando que a média não é igual a 1. 

Para determinar o valor crítico \(K\), com o qual comparamos a
estatística do teste, para tomar a decisão de rejeitar ou não a
hipótese nula, fixamos um \(\alpha = 5\%\). Então se
\(P(|T| > k) = \alpha\), tomamos a decisão de rejeitar \(H_{0}\).
Temos então que o valor de \(K\) é dado por:

\[ K = qt_{(1 - \alpha / 2 ; n - 1)}. \]

Este nada mais é do que o quantil \(1 - \alpha / 2\) da distribuição 
\(T\) com \(n - 1\) graus de liberdade.

No nosso estudo do Erro do tipo I empírico, geraremos as amostras a
partir das distribuições que tem média (verdadeira) \(\mu = 1\).
Faremos então o teste e calcularemos a proporção de vezes que o teste
rejeita \(H_0\). Esta proporção será a estimativa do Erro do Tipo I.

Repetiremos o processo para uma amostra de tamanho
\(n = 20, 100, \text{e } 500\), 10000 mil vezes. Faremos isso para cada
das distribuições e comentaremos então os resultados. 

```{r}
# <r code> ========================================================== #
## Criando a função que gera as amostras,
## calcula a estatística do teste e obtém as taxas do Erro Tipo 1
ex6 <- function(type = "normal", size, mu = 1, m = 10000, alpha = .05){
  t.stat <- replicate(m, {
    switch(type
           , normal = {x <- rnorm(n = size, mean = 1, sd = .25)}
           , chi = {x <- rchisq(n = size, df = 1)}
           , unif = {x <- runif(n = size, min = 0, max = 2)}
           , expo = {x <- rexp(n = size, rate = 1)}
           )
    ( mean(x) - mu ) / ( sd(x) / sqrt(size) )
  })
  mean( t.stat > qt(1 - alpha, size - 1) )
# </r code> ========================================================= #
}
```

```{r, results='hold'}
# <r code> ========================================================== #
## Gerando as amostras,
## calculando a estatística do teste e obtendo as taxas do Erro Tipo 1
set.seed(93)

results <- data.frame(
  `Distribuição` = rep(
    c("Normal", "Chi quadrado", "Uniforme", "Exponencial"), each = 3)
  , `Tamanho amostral` = rep(c("20", "100", "500"), 4)
  , `Taxa empírica` = c(
    unlist(lapply(c(20, 100, 500), ex6, type = "normal"))
    , unlist(lapply(c(20, 100, 500), ex6, type = "chi"))
    , unlist(lapply(c(20, 100, 500), ex6, type = "unif"))
    , unlist(lapply(c(20, 100, 500), ex6, type = "expo"))
  ))
results$`Distribuição` <-
  with(results
       , factor(`Distribuição`, levels(`Distribuição`)[c(3, 1, 4, 2)]))
results$`Tamanho.amostral` <-
  with(results, factor(
    `Tamanho.amostral`, levels(`Tamanho.amostral`)[c(2, 1, 3)]))
results
# </r code> ========================================================= #
```

```{r, fig.width=10, fig.show='hold'}
# <r code> ========================================================== #
## Visualizando os resultados graficamente
library(latticeExtra)

barchart(`Taxa.empírica` ~ `Tamanho.amostral` | `Distribuição`
         , data = results
         , horizontal = FALSE
         , border = "transparent"
         , col = "#0080ff"
         , scales = list(y = list(draw = FALSE))
         , xlab = "Tamanho amostral"
         , ylab = list("Erro Tipo I", rot = 0)
         , main = "Taxa empírica do Erro Tipo I do teste-t"
         , strip = strip.custom(bg = "white")
         , ylim = c(0, max(results$`Taxa.empírica`) + .005)
         , layout = c(4, 1)
         , panel = function(...){
           panel.barchart(...)
           args <- list(...)
           panel.text(args$x, args$y, args$y, pos = 3)
         })
# </r code> ========================================================= #
```

Observe que tanto a distribuição Uniforme (0, 2) quanto a
Normal (1, 0.25) foram as que tiveram os valores mais próximos do valor
esperado \(\alpha = 5\%\). Isso talvez pode ser explicado pela simetria
das distribuições em relação à média. Como a \(\chi^{2}\) (1) e
Exponencial (1) não são simétricas, tiveram resultados mais longe do
esperado. 

É válido ressaltar que a distribuição \(\chi^{2}\) (1) foi a que obteve
a menor taxa de Erro do Tipo I. É possível observar também que com um
aumento do tamanho da amostra, os resultados tendem a ser mais
consistentes e se aproximam mais do valor esperado, para todas as
distribuições analisadas.

# Exercício 7
***

**Determinar uma estimativa de Monte Carlo para:**

$$\int_{0}^{\pi/3}\sin (t) dt$$

**e comparar a estimativa com o valor exato da integral.**

**Solução:**

Primeiramente vamos fazer uma tranformação de variáveis, de modo que o 
novo intervalo de integração seja (0,1)

$$ y = \frac{t-0}{\frac{\pi}{3}-0} = \frac{3t}{\pi}  \\ dy= \frac{3}{\pi} dt$$

Dessa forma, temos que

$$ \int_{0}^{\pi/3}\sin (t) dt = \frac{\pi}{3}\int_{0}^{1}\sin (\frac{y\pi}{3}) dy $$

O que vamos fazer agora é

- Gerar $y_{i}$ com distribuição U[0,1];
- Obter $g(y_{i})= sin(\frac{y_{i}\pi}{3})$;
- Obter a estimativa da integral: 
  $\hat{\theta}=\frac{\pi}{3}g(\bar{y})$;
- Calcular o valor exato da integral.

```{r}
n <- 1e7
y <- runif(n, 0, 1) #gerando U[0,1]
gx <- sin(y*(pi/3)) #aplicando na função
gb <- mean(gx) # calculando a média
theta <- gb*(pi/3) #estimativa da integral pelo método de Monte Carlo
theta

valor_exato  <- (-cos(pi/3)+cos(0)) #Valor exato da integral
valor_exato
```

# Exercício 8
***

**Apresente 2 estimativas de Monte Carlo para**

\[\Phi(x) = P(X \leq x) = \int_{-\infty}^{x}
            \frac{1}{\sqrt{2\pi}} \exp(-t^{2}/2) dt,\]

**também determine estimativas de variância e um IC(95%) para**
\(P(X \leq 1.5)\).

**Solução:**

Pela simetría da distribuição normal padrão essa estimação se reduz a
estimar para $x>0$

\[\begin{equation} 
\theta=\int_0^x \dfrac{1}{\sqrt{2\pi}}\exp(-t^2/2) dt,
\end{equation}\]

vamos obter duas estimativas de $\theta$,  a primeira usando uma a.a. 
de uniformes em (0,1) e outra gerando uniformes em $(0,x).$, para um 
valor de $x>0.$ previamente fixado. Finalmente para obtermos as 
estimativas para $\Phi(x)$ usamos

$$\widehat{\Phi(x)}=\hat{\theta}+0.5,$$
a implementação do algoritmo usando estimação por Monte Carlo é
apresentado a continuação, para o valor $x=1.5$. 

```{r}
#usando uniformes (0,1)

m <-1000;
n <-200;
x <-1.5;
alpha=0.05;
cdf <- replicate(m, {
  u <-runif(n)
  g<-(x*exp((-(u*x)^2)/2))/sqrt(2*pi)
  mean(g)+0.5})
media1 <- mean(cdf);
v1<- var(cdf);
ic1<-round(media1 + c(-1,1) * qnorm(1-alpha/2,mean=0,sd=1) * sqrt(v1/m), 4)
print(media1)
print(v1)
print(ic1)
```

```{r}
#Usando uniformes (0,x)

x <- 1.5;
n <-1000;
m <-200;
alpha=0.05;
cdf <- replicate(m, {
  u <-runif(n, 0, x)
  g<-(exp((-(u)^2)/2))/sqrt(2*pi)
  x*mean(g)+0.5})
media2 <- mean(cdf);
v2<- var(cdf);
ic2<-round(media2 + c(-1,1) * qnorm(1-alpha/2,mean=0,sd=1) *sqrt(v2/m), 4)
print(media2)
print(v2)
print(ic2)
```

# Exercício 9
***

**Use o Método de Variáveis Antitéticas para estimar**

$$\int_{0}^{1} \frac{e^{-x}} {1+x^2} dx$$

**e determine o percentual de redução de variância quando compara-se
este resultado àquele sem a técnica de redução de variância.**

**Solução:**

Queremos estimar 

$$\theta= E\Big[\frac{e^{-U}} {1+U^2}\Big] = \int_{0}^{1} \frac{e^{-u}}
{1+u^2} du$$

expressão na qual $U \sim (0,1)$.

Se $m$ replicatas de Monte Carlo são necessárias, então:

1. Gere $u_1,…,u_{m/2} \sim U(0,1)$

```{r}
m = 1e6
U = runif(m/2)
```

2. Gere $Y_i=\frac{e^{-u}} {1+u^2}$

3. Gere  $W_i=\frac{e^{-(1-u)}} {1+(1-u)^2}$

4. Retornar $\widehat{\theta}=\frac{2}{m}\sum_{i=1}^{m/2} (\frac{Y_i+W_i}{2})$.

```{r}
theta.estimado <- function(U){

  y =exp(-U)/(1+U^2)
  w =exp(U-1)/(1+(1-U)^2)
  return(mean((y+w)/2))
}

theta.estimado(U)
```

Assim encontramos a estimativa de `r theta.estimado(U)` para $\theta$.

Para comparar o estimador encontado com aquele sem a técnica de
redução de variância replicamos 1000 vezes a simulação e calculamos a
variância amostral para cada estimador.

```{r}
theta.sem.red <- function(U){
  fx =exp(-U)/(1+U^2)
  return(mean(fx))
}
mc1 = replicate(1000, theta.sem.red(runif(1000)))
mc2 = replicate(1000, theta.estimado(runif(1000)))

```

Seja $\widehat{\theta}_1$ o estimador encontrado sem a técnica de
redução de variância e $\widehat{\theta}_2$ o estimador com redução de
variância, encontramos que $SD(\widehat{\theta}_1)$=
`r round(sd(mc1),5)` e $SD(\widehat{\theta}_2)$=`r round(sd(mc2),5)`.

Se temos 2 estimadores, $θ_1$ e  $θ_2$, e $V(θ_2)<V(θ_1)$, então o
percentual de redução de variância é:

$$ 100 (\frac{V(\widehat{\theta}_1)-V(\widehat{\theta}_2)}
{V(\widehat{\theta}_1)})\%$$

Portanto, temos que o percentual de redução de variância será de
`r round(100*(var(mc1)-var(mc2))/var(mc1), 2)`% :

```{r}
round(100*(var(mc1)-var(mc2))/var(mc1), 2)
```

# Exercício 10
***

**Estime**

\[ E(X^{2}) = \int_{1}^{\infty} \frac{x^{2}}{\sqrt{2\pi}}
               \exp\left( \frac{-x^{2}}{2} \right) dx \]

**utilizando amostragem por importância.**

A questão pede que estimemos a esperança de $X^2$ de uma distribuição
normal padrão definida no intervalo $[1,\infty)$ pelo método da
amostragem por importância.

Uma distribuição interessante para ser utilizada é a distribuição
normal padrão truncada em 1, porque possui o mesmo suporte que a
densidade que temos $[0,\infty)$ e, além disso, a razão entre ambas
gera uma expressão estável. A estabilidade se dá pela semelhança entre
a distribuição normal que a questão oferece e a distribuição normal
truncada. 

A razão referida anteriormente pode ser definida, com alguma
simplificação de notação, da seguinte maneira: 

$$
R = \frac{g(x)}{f(x)},
$$

onde $g(x)$ denota a distribuição original, no caso a distribuição
normal padrão pré multiplicada por $x^2$ e $f(X)$ a função de
importância. É importante observar que a variância do estimador
depende de $R$, logo é aconselhável obter razões $R$ mais estáveis.

A implementação do algoritmo de importância segue abaixo:

```{r}
MCNormTruncX2 <- function(n,lower=1,upper=Inf,media=0,dp=1){
  # verificando se o pacote truncnorm precisa ser instalado
  if("truncnorm" %in% rownames(installed.packages())==FALSE)
  {install.packages("truncnorm");library(truncnorm)}
  else library(truncnorm)
  
  rtn <- rtruncnorm(n=n,a=lower,b=upper,mean=media,sd=dp)
  int1 <- ((rtn^2)*dnorm(x=rtn,mean=media,sd=dp))/
    dtruncnorm(x=rtn, a=lower,b=upper,mean=media,sd=dp)
  impMC1 <- mean(int1)
  varinf1 <- var(int1)/n
  
  result <- list(thetahat = impMC1,
                 varhat = varinf1)
  return(result)
}
cbind(rbind(MCNormTruncX2(n=100),MCNormTruncX2(n=1e4)
            ,MCNormTruncX2(n=1e6)),c(100,1e4,1e6))
```

Note que os valores de truncagem estão pré definidos, mas nada impede
que os valores de $lower$ e $upper$ sejam substituidos. O mesmo ocorre
com os valores da média e do desvio-padrão. Note também que os
resultados indicam que o estimador é consistente, pois à medida que se
eleva $n$, a variância do estimador reduz consideravelmente.

***
***
